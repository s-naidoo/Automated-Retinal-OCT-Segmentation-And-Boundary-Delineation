{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STslWxw8WPQs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYXogm8FwuUm"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# 1. Mount Google Drive\n",
        "##############################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "#######################################################\n",
        "# 2. Select whether to train or load an existing model\n",
        "#######################################################\n",
        "TRAIN_FROM_SCRATCH = False\n",
        "LOAD_CHECKPOINT_PATH = \"/content/drive/MyDrive/TrainedModels/UNET/final_model_unetpp_comb_ep_45.pth\"\n",
        "FINAL_MODEL_SAVE_PATH = \"/content/drive/MyDrive/TrainedModels/UNET/final_model_unetpp_comb_ep_45.pth\"\n",
        "\n",
        "##############################################\n",
        "# 2.1 Choose which architecture to use\n",
        "##############################################\n",
        "# Possible values: \"unet\", \"unet++\", \"unet3+\"\n",
        "MODEL_ARCHITECTURE = \"unet++\"\n",
        "\n",
        "##############################################\n",
        "# 2.2 Choose which loss function to use\n",
        "##############################################\n",
        "# Possible values: \"CE\", \"Dice\", \"Combined\"\n",
        "LOSS_TYPE = \"Combined\"  # For example, Cross Entropy only\n",
        "\n",
        "#####################################\n",
        "# 3. Imports & Global Setup\n",
        "#####################################\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.patches import Patch\n",
        "import matplotlib.cm as cm\n",
        "import time\n",
        "import datetime\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import scipy.ndimage as ndimage\n",
        "\n",
        "# Use serif font for plots\n",
        "matplotlib.rc('font', family='serif', serif=['DejaVu Serif'])\n",
        "matplotlib.rc('mathtext', fontset='cm')\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "#####################################\n",
        "# 4. Create root output directories\n",
        "#####################################\n",
        "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "ROOT_OUTPUT_DIR = f\"/content/drive/MyDrive/final_model_unetpp_comb_ep_45_metrics\"\n",
        "os.makedirs(ROOT_OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Created root output directory: {ROOT_OUTPUT_DIR}\")\n",
        "\n",
        "SIGNAL_ERROR_OUTPUT_DIR = os.path.join(ROOT_OUTPUT_DIR, \"SignalErrors\")\n",
        "DURATIONS_OUTPUT_DIR = os.path.join(ROOT_OUTPUT_DIR, \"TrainingDurations\")\n",
        "EPOCH_METRICS_OUTPUT_DIR = os.path.join(ROOT_OUTPUT_DIR, \"EpochMetrics\")\n",
        "FINAL_METRICS_OUTPUT_DIR = os.path.join(ROOT_OUTPUT_DIR, \"FinalMetrics\")\n",
        "TOTAL_TRAIN_TIME_OUTPUT_DIR = os.path.join(ROOT_OUTPUT_DIR, \"TotalTrainTime\")\n",
        "\n",
        "for d in [\n",
        "    SIGNAL_ERROR_OUTPUT_DIR, DURATIONS_OUTPUT_DIR,\n",
        "    EPOCH_METRICS_OUTPUT_DIR, FINAL_METRICS_OUTPUT_DIR,\n",
        "    TOTAL_TRAIN_TIME_OUTPUT_DIR\n",
        "]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "#####################################\n",
        "# 5. Custom Variables\n",
        "#####################################\n",
        "custom_colors = [\n",
        "    '#000075', '#C2185B', '#FF1744', '#FF5722', '#FF9800',\n",
        "    '#FFC107', '#CDDC39', '#8BC34A', '#009688', '#2979FF'\n",
        "]\n",
        "custom_class_names = [\n",
        "    \"Background\", \"RNFL\", \"GCL+IPL\", \"INL\", \"OPL\",\n",
        "    \"ONL\", \"IS\", \"OS\", \"RPE\", \"Choroid\"\n",
        "]\n",
        "custom_boundary_colors = [\n",
        "    '#C2185B', '#FF1744', '#FF5722', '#FF9800', '#FFC107',\n",
        "    '#CDDC39', '#8BC34A', '#009688', '#2979FF'\n",
        "]\n",
        "custom_cmap = mcolors.ListedColormap(custom_colors)\n",
        "\n",
        "VALIDATION_CUSTOM_X_LIMITS = None\n",
        "VALIDATION_CUSTOM_Y_LIMITS = None\n",
        "TEST_CUSTOM_X_LIMITS = None\n",
        "TEST_CUSTOM_Y_LIMITS = None\n",
        "VALIDATION_OVERALL_CUSTOM_X_LIMITS = None\n",
        "VALIDATION_OVERALL_CUSTOM_Y_LIMITS = None\n",
        "TEST_OVERALL_CUSTOM_X_LIMITS = None\n",
        "TEST_OVERALL_CUSTOM_Y_LIMITS = None\n",
        "\n",
        "#####################################\n",
        "# 6. Data Augmentations & Preprocessing\n",
        "#####################################\n",
        "class RandomHorizontalFlip:\n",
        "    def __init__(self, p=0.5):\n",
        "        self.p = p\n",
        "    def __call__(self, image, mask):\n",
        "        if random.random() < self.p:\n",
        "            image = np.flip(image, axis=2).copy()\n",
        "            mask  = np.flip(mask, axis=1).copy()\n",
        "        return image, mask\n",
        "\n",
        "class RandomElasticDeform:\n",
        "    def __init__(self, alpha=34, sigma=4, p=0.5):\n",
        "        self.alpha = alpha\n",
        "        self.sigma = sigma\n",
        "        self.p = p\n",
        "    def __call__(self, image, mask):\n",
        "        if random.random() < self.p:\n",
        "            c, h, w = image.shape\n",
        "            dx = ndimage.gaussian_filter((np.random.rand(h, w)*2 - 1),\n",
        "                                         self.sigma, mode=\"reflect\") * self.alpha\n",
        "            dy = ndimage.gaussian_filter((np.random.rand(h, w)*2 - 1),\n",
        "                                         self.sigma, mode=\"reflect\") * self.alpha\n",
        "            x, y = np.meshgrid(np.arange(w), np.arange(h))\n",
        "            map_x = (x + dx).astype(np.float32)\n",
        "            map_y = (y + dy).astype(np.float32)\n",
        "\n",
        "            image_2d = image[0]\n",
        "            distorted_image_2d = cv2.remap(image_2d, map_x, map_y,\n",
        "                                           interpolation=cv2.INTER_LINEAR,\n",
        "                                           borderMode=cv2.BORDER_REFLECT_101)\n",
        "            distorted_mask = cv2.remap(mask.astype(np.float32), map_x, map_y,\n",
        "                                       interpolation=cv2.INTER_NEAREST,\n",
        "                                       borderMode=cv2.BORDER_REFLECT_101).astype(mask.dtype)\n",
        "\n",
        "            distorted_image = np.expand_dims(distorted_image_2d, axis=0)\n",
        "            return distorted_image, distorted_mask\n",
        "        return image, mask\n",
        "\n",
        "class RandomBrightnessContrast:\n",
        "    def __init__(self, brightness_range=0.2, contrast_range=0.2, p=0.5):\n",
        "        self.brightness_range = brightness_range\n",
        "        self.contrast_range = contrast_range\n",
        "        self.p = p\n",
        "    def __call__(self, image, mask):\n",
        "        if random.random() < self.p:\n",
        "            brightness_shift = random.uniform(-self.brightness_range, self.brightness_range)\n",
        "            contrast_factor = random.uniform(1 - self.contrast_range, 1 + self.contrast_range)\n",
        "            image = image + brightness_shift\n",
        "            image = image * contrast_factor\n",
        "        return image, mask\n",
        "\n",
        "class CLAHE_ZScore:\n",
        "    def __init__(self, clipLimit=2.0, tileGridSize=(8,8)):\n",
        "        self.clahe = cv2.createCLAHE(clipLimit=clipLimit, tileGridSize=tileGridSize)\n",
        "    def __call__(self, image, mask):\n",
        "        image_uint8 = np.clip(image[0], 0, 255).astype(np.uint8)\n",
        "        clahe_img = self.clahe.apply(image_uint8)\n",
        "        clahe_img = clahe_img.astype(np.float32)\n",
        "        mean = np.mean(clahe_img)\n",
        "        std = np.std(clahe_img)\n",
        "        if std < 1e-6:\n",
        "            std = 1e-6\n",
        "        normalized = (clahe_img - mean) / std\n",
        "        normalized = np.expand_dims(normalized, axis=0)\n",
        "        return normalized, mask\n",
        "\n",
        "class RandomSimulatedArtifacts:\n",
        "    def __init__(self, p=0.5, artifact_type='occlusion'):\n",
        "        self.p = p\n",
        "        self.artifact_type = artifact_type\n",
        "    def __call__(self, image, mask):\n",
        "        if random.random() < self.p:\n",
        "            c, h, w = image.shape\n",
        "            if self.artifact_type == 'occlusion':\n",
        "                rect_w = random.randint(w//10, w//4)\n",
        "                rect_h = random.randint(h//10, h//4)\n",
        "                x1 = random.randint(0, w - rect_w)\n",
        "                y1 = random.randint(0, h - rect_h)\n",
        "                image[:, y1:y1+rect_h, x1:x1+rect_w] = 0\n",
        "            elif self.artifact_type == 'gaussian_noise':\n",
        "                noise = np.random.normal(0, 0.1, size=image.shape).astype(np.float32)\n",
        "                image = image + noise\n",
        "        return image, mask\n",
        "\n",
        "class ComposeAugmentations:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "    def __call__(self, image, mask):\n",
        "        for t in self.transforms:\n",
        "            image, mask = t(image, mask)\n",
        "        return image, mask\n",
        "\n",
        "#########################################\n",
        "# 7. OCTDataset Class\n",
        "#########################################\n",
        "class OCTDataset(Dataset):\n",
        "    def __init__(self, images_dir, masks_dir, transform=None, target_size=(1024,256)):\n",
        "        self.images_dir = images_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.transform = transform\n",
        "        self.target_size = target_size\n",
        "        self.image_names = sorted(os.listdir(images_dir))\n",
        "        self.mask_names = sorted(os.listdir(masks_dir))\n",
        "        assert len(self.image_names) == len(self.mask_names), (\n",
        "            f\"Mismatch: {len(self.image_names)} vs {len(self.mask_names)}\"\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.images_dir, self.image_names[idx])\n",
        "        msk_path = os.path.join(self.masks_dir, self.mask_names[idx])\n",
        "\n",
        "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        orig_h, orig_w = image.shape[:2]\n",
        "\n",
        "        image = cv2.resize(image, self.target_size, interpolation=cv2.INTER_AREA)\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "        with open(msk_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        boundaries = np.array(data[\"bds\"], dtype=np.float32)\n",
        "        num_boundaries = boundaries.shape[0]\n",
        "\n",
        "        label_mask_full = np.zeros((orig_h, orig_w), dtype=np.int64)\n",
        "        for col_idx in range(orig_w):\n",
        "            col_bounds = np.clip(boundaries[:, col_idx], 0, orig_h-1).astype(int)\n",
        "            col_bounds = np.sort(col_bounds)\n",
        "            start_row = 0\n",
        "            for class_id, b_row in enumerate(col_bounds):\n",
        "                label_mask_full[start_row:b_row, col_idx] = class_id\n",
        "                start_row = b_row\n",
        "            label_mask_full[start_row:orig_h, col_idx] = num_boundaries\n",
        "\n",
        "        label_mask = cv2.resize(label_mask_full, self.target_size, interpolation=cv2.INTER_NEAREST)\n",
        "        label_mask = label_mask.astype(np.int64)\n",
        "\n",
        "        if self.transform:\n",
        "            image, label_mask = self.transform(image, label_mask)\n",
        "\n",
        "        image_tensor = torch.from_numpy(image).float()\n",
        "        label_tensor = torch.from_numpy(label_mask).long()\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "##########################################\n",
        "# 8. Create DataLoaders and Transforms\n",
        "##########################################\n",
        "train_transform = ComposeAugmentations([\n",
        "    CLAHE_ZScore(),\n",
        "    RandomSimulatedArtifacts(p=0.5, artifact_type='occlusion'),\n",
        "    RandomHorizontalFlip(p=0.5),\n",
        "    RandomBrightnessContrast(brightness_range=0.2, contrast_range=0.2, p=0.5),\n",
        "    RandomElasticDeform(alpha=34, sigma=4, p=0.5),\n",
        "])\n",
        "\n",
        "val_transform = ComposeAugmentations([CLAHE_ZScore()])\n",
        "test_transform = ComposeAugmentations([CLAHE_ZScore()])\n",
        "\n",
        "train_images_dir = \"/content/drive/MyDrive/RetinaOCTData/TrainImages\"\n",
        "train_masks_dir = \"/content/drive/MyDrive/RetinaOCTData/TrainMasks\"\n",
        "val_images_dir = \"/content/drive/MyDrive/RetinaOCTData/ValImages\"\n",
        "val_masks_dir = \"/content/drive/MyDrive/RetinaOCTData/ValMasks\"\n",
        "test_images_dir = \"/content/drive/MyDrive/RetinaOCTData/TestImages\"\n",
        "test_masks_dir = \"/content/drive/MyDrive/RetinaOCTData/TestMasks\"\n",
        "\n",
        "train_dataset = OCTDataset(train_images_dir, train_masks_dir,\n",
        "                           transform=train_transform, target_size=(1024,256))\n",
        "val_dataset = OCTDataset(val_images_dir, val_masks_dir,\n",
        "                         transform=val_transform, target_size=(1024,256))\n",
        "test_dataset = OCTDataset(test_images_dir, test_masks_dir,\n",
        "                          transform=test_transform, target_size=(1024,256))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True,\n",
        "                          num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,\n",
        "                         num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Total training samples:\", len(train_dataset))\n",
        "print(\"Total validation samples:\", len(val_dataset))\n",
        "print(\"Total test samples:\", len(test_dataset))\n",
        "\n",
        "#############################################\n",
        "# 9. Define DoubleConv, UNet, UNet++, UNet3+\n",
        "#############################################\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Classic UNet-style Double Convolution:\n",
        "    (Conv -> BN -> ReLU) repeated twice.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A 5-level classic U-Net with DoubleConv blocks.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=1, num_classes=10, base_filters=64):\n",
        "        super(UNet, self).__init__()\n",
        "        f = [\n",
        "            base_filters,\n",
        "            base_filters * 2,\n",
        "            base_filters * 4,\n",
        "            base_filters * 8,\n",
        "            base_filters * 16,\n",
        "        ]\n",
        "\n",
        "        self.encoder1 = DoubleConv(in_channels, f[0])\n",
        "        self.pool1    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder2 = DoubleConv(f[0], f[1])\n",
        "        self.pool2    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder3 = DoubleConv(f[1], f[2])\n",
        "        self.pool3    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.encoder4 = DoubleConv(f[2], f[3])\n",
        "        self.pool4    = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.bottleneck = DoubleConv(f[3], f[4])\n",
        "\n",
        "        self.up4  = nn.ConvTranspose2d(f[4], f[3], kernel_size=2, stride=2)\n",
        "        self.dec4 = DoubleConv(f[3]*2, f[3])\n",
        "        self.up3  = nn.ConvTranspose2d(f[3], f[2], kernel_size=2, stride=2)\n",
        "        self.dec3 = DoubleConv(f[2]*2, f[2])\n",
        "        self.up2  = nn.ConvTranspose2d(f[2], f[1], kernel_size=2, stride=2)\n",
        "        self.dec2 = DoubleConv(f[1]*2, f[1])\n",
        "        self.up1  = nn.ConvTranspose2d(f[1], f[0], kernel_size=2, stride=2)\n",
        "        self.dec1 = DoubleConv(f[0]*2, f[0])\n",
        "\n",
        "        self.final_conv = nn.Conv2d(f[0], num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.encoder1(x)\n",
        "        x1p = self.pool1(x1)\n",
        "        x2 = self.encoder2(x1p)\n",
        "        x2p = self.pool2(x2)\n",
        "        x3 = self.encoder3(x2p)\n",
        "        x3p = self.pool3(x3)\n",
        "        x4 = self.encoder4(x3p)\n",
        "        x4p = self.pool4(x4)\n",
        "\n",
        "        x5 = self.bottleneck(x4p)\n",
        "\n",
        "        d4 = self.up4(x5)\n",
        "        d4 = torch.cat([x4, d4], dim=1)\n",
        "        d4 = self.dec4(d4)\n",
        "\n",
        "        d3 = self.up3(d4)\n",
        "        d3 = torch.cat([x3, d3], dim=1)\n",
        "        d3 = self.dec3(d3)\n",
        "\n",
        "        d2 = self.up2(d3)\n",
        "        d2 = torch.cat([x2, d2], dim=1)\n",
        "        d2 = self.dec2(d2)\n",
        "\n",
        "        d1 = self.up1(d2)\n",
        "        d1 = torch.cat([x1, d1], dim=1)\n",
        "        d1 = self.dec1(d1)\n",
        "\n",
        "        out = self.final_conv(d1)\n",
        "        return out\n",
        "\n",
        "class UNetPlusPlus(nn.Module):\n",
        "    \"\"\"\n",
        "    A 5-level UNet++ with properly fixed upsampling channels.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=1, num_classes=10, base_filters=64):\n",
        "        super(UNetPlusPlus, self).__init__()\n",
        "        self.filters = [\n",
        "            base_filters,\n",
        "            base_filters * 2,\n",
        "            base_filters * 4,\n",
        "            base_filters * 8,\n",
        "            base_filters * 16,\n",
        "        ]\n",
        "\n",
        "        self.conv0_0 = DoubleConv(in_channels, self.filters[0])\n",
        "        self.conv1_0 = DoubleConv(self.filters[0], self.filters[1])\n",
        "        self.conv2_0 = DoubleConv(self.filters[1], self.filters[2])\n",
        "        self.conv3_0 = DoubleConv(self.filters[2], self.filters[3])\n",
        "        self.conv4_0 = DoubleConv(self.filters[3], self.filters[4])\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.up0_1 = nn.ConvTranspose2d(self.filters[1], self.filters[0], kernel_size=2, stride=2)\n",
        "        self.up1_1 = nn.ConvTranspose2d(self.filters[2], self.filters[1], kernel_size=2, stride=2)\n",
        "        self.up2_1 = nn.ConvTranspose2d(self.filters[3], self.filters[2], kernel_size=2, stride=2)\n",
        "        self.up3_1 = nn.ConvTranspose2d(self.filters[4], self.filters[3], kernel_size=2, stride=2)\n",
        "\n",
        "        self.up0_2 = nn.ConvTranspose2d(self.filters[1], self.filters[0], kernel_size=2, stride=2)\n",
        "        self.up1_2 = nn.ConvTranspose2d(self.filters[2], self.filters[1], kernel_size=2, stride=2)\n",
        "        self.up2_2 = nn.ConvTranspose2d(self.filters[3], self.filters[2], kernel_size=2, stride=2)\n",
        "\n",
        "        self.up0_3 = nn.ConvTranspose2d(self.filters[1], self.filters[0], kernel_size=2, stride=2)\n",
        "        self.up1_3 = nn.ConvTranspose2d(self.filters[2], self.filters[1], kernel_size=2, stride=2)\n",
        "\n",
        "        self.up0_4 = nn.ConvTranspose2d(self.filters[1], self.filters[0], kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv0_1 = DoubleConv(self.filters[0]*2, self.filters[0])\n",
        "        self.conv1_1 = DoubleConv(self.filters[1]*2, self.filters[1])\n",
        "        self.conv2_1 = DoubleConv(self.filters[2]*2, self.filters[2])\n",
        "        self.conv3_1 = DoubleConv(self.filters[3]*2, self.filters[3])\n",
        "\n",
        "        self.conv0_2 = DoubleConv(self.filters[0]*3, self.filters[0])\n",
        "        self.conv1_2 = DoubleConv(self.filters[1]*3, self.filters[1])\n",
        "        self.conv2_2 = DoubleConv(self.filters[2]*3, self.filters[2])\n",
        "\n",
        "        self.conv0_3 = DoubleConv(self.filters[0]*4, self.filters[0])\n",
        "        self.conv1_3 = DoubleConv(self.filters[1]*4, self.filters[1])\n",
        "\n",
        "        self.conv0_4 = DoubleConv(self.filters[0]*5, self.filters[0])\n",
        "\n",
        "        self.final_conv = nn.Conv2d(self.filters[0], num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0_0 = self.conv0_0(x)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "\n",
        "        # x0_1\n",
        "        x0_1_up = self.up0_1(x1_0)\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, x0_1_up], dim=1))\n",
        "\n",
        "        # x1_1\n",
        "        x1_1_up = self.up1_1(x2_0)\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, x1_1_up], dim=1))\n",
        "\n",
        "        # x2_1\n",
        "        x2_1_up = self.up2_1(x3_0)\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, x2_1_up], dim=1))\n",
        "\n",
        "        # x3_1\n",
        "        x3_1_up = self.up3_1(x4_0)\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, x3_1_up], dim=1))\n",
        "\n",
        "        # x0_2\n",
        "        x0_2_up = self.up0_2(x1_1)\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, x0_2_up], dim=1))\n",
        "\n",
        "        # x1_2\n",
        "        x1_2_up = self.up1_2(x2_1)\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, x1_2_up], dim=1))\n",
        "\n",
        "        # x2_2\n",
        "        x2_2_up = self.up2_2(x3_1)\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, x2_2_up], dim=1))\n",
        "\n",
        "        # x0_3\n",
        "        x0_3_up = self.up0_3(x1_2)\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, x0_3_up], dim=1))\n",
        "\n",
        "        # x1_3\n",
        "        x1_3_up = self.up1_3(x2_2)\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, x1_3_up], dim=1))\n",
        "\n",
        "        # x0_4\n",
        "        x0_4_up = self.up0_4(x1_3)\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, x0_4_up], dim=1))\n",
        "\n",
        "        out = self.final_conv(x0_4)\n",
        "        return out\n",
        "\n",
        "class UNet3Plus(nn.Module):\n",
        "    \"\"\"\n",
        "    A 5-level UNet 3+ with multi-scale feature aggregation + deep supervision,\n",
        "    so we have actual parameters to train.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=1, num_classes=10, base_filters=64):\n",
        "        super(UNet3Plus, self).__init__()\n",
        "        self.f = [\n",
        "            base_filters,\n",
        "            base_filters * 2,\n",
        "            base_filters * 4,\n",
        "            base_filters * 8,\n",
        "            base_filters * 16,\n",
        "        ]\n",
        "\n",
        "        self.conv1 = DoubleConv(in_channels, self.f[0])\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = DoubleConv(self.f[0], self.f[1])\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = DoubleConv(self.f[1], self.f[2])\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)\n",
        "        self.conv4 = DoubleConv(self.f[2], self.f[3])\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = DoubleConv(self.f[3], self.f[4])\n",
        "\n",
        "        agg_in_ch = sum(self.f)  # 64 + 128 + 256 + 512 + 1024 = 1984\n",
        "\n",
        "        self.conv_x4d = DoubleConv(agg_in_ch, self.f[3])\n",
        "        self.conv_x3d = DoubleConv(agg_in_ch, self.f[2])\n",
        "        self.conv_x2d = DoubleConv(agg_in_ch, self.f[1])\n",
        "        self.conv_x1d = DoubleConv(agg_in_ch, self.f[0])\n",
        "\n",
        "        self.out_conv4 = nn.Conv2d(self.f[3], num_classes, kernel_size=1)\n",
        "        self.out_conv3 = nn.Conv2d(self.f[2], num_classes, kernel_size=1)\n",
        "        self.out_conv2 = nn.Conv2d(self.f[1], num_classes, kernel_size=1)\n",
        "        self.out_conv1 = nn.Conv2d(self.f[0], num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = self.conv2(self.pool1(x1))\n",
        "        x3 = self.conv3(self.pool2(x2))\n",
        "        x4 = self.conv4(self.pool3(x3))\n",
        "        x5 = self.conv5(self.pool4(x4))\n",
        "\n",
        "        size1 = x1.size()[2:]\n",
        "        size2 = x2.size()[2:]\n",
        "        size3 = x3.size()[2:]\n",
        "        size4 = x4.size()[2:]\n",
        "\n",
        "        x4_d_in = torch.cat([\n",
        "            F.interpolate(x1, size=size4, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x2, size=size4, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x3, size=size4, mode='bilinear', align_corners=True),\n",
        "            x4,\n",
        "            F.interpolate(x5, size=size4, mode='bilinear', align_corners=True)\n",
        "        ], dim=1)\n",
        "        x4_d = self.conv_x4d(x4_d_in)\n",
        "\n",
        "        x3_d_in = torch.cat([\n",
        "            F.interpolate(x1, size=size3, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x2, size=size3, mode='bilinear', align_corners=True),\n",
        "            x3,\n",
        "            F.interpolate(x4_d, size=size3, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x5, size=size3, mode='bilinear', align_corners=True)\n",
        "        ], dim=1)\n",
        "        x3_d = self.conv_x3d(x3_d_in)\n",
        "\n",
        "        x2_d_in = torch.cat([\n",
        "            F.interpolate(x1, size=size2, mode='bilinear', align_corners=True),\n",
        "            x2,\n",
        "            F.interpolate(x3_d, size=size2, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x4_d, size=size2, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x5, size=size2, mode='bilinear', align_corners=True)\n",
        "        ], dim=1)\n",
        "        x2_d = self.conv_x2d(x2_d_in)\n",
        "\n",
        "        x1_d_in = torch.cat([\n",
        "            x1,\n",
        "            F.interpolate(x2_d, size=size1, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x3_d, size=size1, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x4_d, size=size1, mode='bilinear', align_corners=True),\n",
        "            F.interpolate(x5, size=size1, mode='bilinear', align_corners=True)\n",
        "        ], dim=1)\n",
        "        x1_d = self.conv_x1d(x1_d_in)\n",
        "\n",
        "        side4 = self.out_conv4(x4_d)\n",
        "        side3 = self.out_conv3(x3_d)\n",
        "        side2 = self.out_conv2(x2_d)\n",
        "        side1 = self.out_conv1(x1_d)\n",
        "\n",
        "        side4_up = F.interpolate(side4, size=size1, mode='bilinear', align_corners=True)\n",
        "        side3_up = F.interpolate(side3, size=size1, mode='bilinear', align_corners=True)\n",
        "        side2_up = F.interpolate(side2, size=size1, mode='bilinear', align_corners=True)\n",
        "\n",
        "        final = (side1 + side2_up + side3_up + side4_up) / 4.0\n",
        "        return final\n",
        "\n",
        "#############################################\n",
        "# 10. Metric Functions and Loss Definitions\n",
        "#############################################\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "class MultiClassDiceLoss(nn.Module):\n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(MultiClassDiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "    def forward(self, logits, targets):\n",
        "        probs = nn.functional.softmax(logits, dim=1)\n",
        "        num_classes = logits.shape[1]\n",
        "        targets_one_hot = nn.functional.one_hot(targets, num_classes=num_classes)\n",
        "        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()\n",
        "        intersection = torch.sum(probs * targets_one_hot, dim=(0,2,3))\n",
        "        union = torch.sum(probs, dim=(0,2,3)) + torch.sum(targets_one_hot, dim=(0,2,3))\n",
        "        dice_per_class = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
        "        dice_score = torch.mean(dice_per_class)\n",
        "        dice_loss_val = 1.0 - dice_score\n",
        "        return dice_loss_val\n",
        "\n",
        "dice_loss = MultiClassDiceLoss(smooth=1.0)\n",
        "\n",
        "def combined_loss(logits, targets, alpha=0.5):\n",
        "    loss_ce = ce_loss(logits, targets)\n",
        "    loss_d = dice_loss(logits, targets)\n",
        "    return alpha * loss_ce + (1.0 - alpha) * loss_d\n",
        "\n",
        "def create_confusion_matrix(num_classes):\n",
        "    return np.zeros((num_classes, num_classes), dtype=np.float64)\n",
        "\n",
        "def update_confusion_matrix(pred, target, conf_mat):\n",
        "    pred_np = pred.ravel()\n",
        "    target_np = target.ravel()\n",
        "    for t, p in zip(target_np, pred_np):\n",
        "        if 0 <= t < conf_mat.shape[0] and 0 <= p < conf_mat.shape[0]:\n",
        "            conf_mat[t, p] += 1\n",
        "\n",
        "def compute_classwise_iou_dice(conf_mat):\n",
        "    C = conf_mat.shape[0]\n",
        "    ious, dices = [], []\n",
        "    for c in range(C):\n",
        "        tp = conf_mat[c, c]\n",
        "        fp = conf_mat[:, c].sum() - tp\n",
        "        fn = conf_mat[c, :].sum() - tp\n",
        "        denom_iou = tp + fp + fn\n",
        "        iou = tp / denom_iou if denom_iou > 1e-10 else 0\n",
        "        ious.append(iou)\n",
        "        denom_dice = (2 * tp) + fp + fn\n",
        "        dice = (2 * tp) / denom_dice if denom_dice > 1e-10 else 0\n",
        "        dices.append(dice)\n",
        "    mean_iou = float(np.mean(ious))\n",
        "    mean_dice = float(np.mean(dices))\n",
        "    return ious, dices, mean_iou, mean_dice\n",
        "\n",
        "def compute_precision_recall_f1(conf_mat):\n",
        "    C = conf_mat.shape[0]\n",
        "    precisions, recalls, f1s = [], [], []\n",
        "    for c in range(C):\n",
        "        tp = conf_mat[c, c]\n",
        "        fp = conf_mat[:, c].sum() - tp\n",
        "        fn = conf_mat[c, :].sum() - tp\n",
        "        prec = tp / (tp + fp + 1e-15)\n",
        "        rec = tp / (tp + fn + 1e-15)\n",
        "        f1 = 2 * prec * rec / (prec + rec + 1e-15)\n",
        "        precisions.append(prec)\n",
        "        recalls.append(rec)\n",
        "        f1s.append(f1)\n",
        "    macro_prec = float(np.mean(precisions))\n",
        "    macro_rec = float(np.mean(recalls))\n",
        "    macro_f1 = float(np.mean(f1s))\n",
        "    return (precisions, recalls, f1s, macro_prec, macro_rec, macro_f1)\n",
        "\n",
        "def extract_boundaries(label_map, num_boundaries=9):\n",
        "    H, W = label_map.shape\n",
        "    boundaries = np.full((num_boundaries, W), H-1, dtype=np.float32)\n",
        "    for c in range(W):\n",
        "        col_vals = label_map[:, c]\n",
        "        for i in range(num_boundaries):\n",
        "            indices = np.where((col_vals[:-1] == i) & (col_vals[1:] == i+1))[0]\n",
        "            if indices.size > 0:\n",
        "                boundaries[i, c] = indices[0] + 1\n",
        "    return boundaries\n",
        "\n",
        "def single_sample_metrics(pred_np, target_np, num_classes=10):\n",
        "    correct_pixels = np.sum(pred_np == target_np)\n",
        "    total_pixels = target_np.size\n",
        "    sample_pix_acc = correct_pixels / total_pixels\n",
        "    sample_conf = np.zeros((num_classes, num_classes), dtype=np.float64)\n",
        "    for t, p in zip(target_np.ravel(), pred_np.ravel()):\n",
        "        if 0 <= t < num_classes and 0 <= p < num_classes:\n",
        "            sample_conf[t, p] += 1\n",
        "    ious, _, mean_iou, _ = compute_classwise_iou_dice(sample_conf)\n",
        "    return sample_pix_acc, mean_iou\n",
        "\n",
        "#############################################\n",
        "# 11. Train & Validate Loop\n",
        "#############################################\n",
        "def train_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in loader:\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    return running_loss / len(loader)\n",
        "\n",
        "def validate_epoch(model, loader, criterion, num_classes=10, num_boundaries=9):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_pixels = 0\n",
        "    conf_mat = create_confusion_matrix(num_classes)\n",
        "    sample_rmse_list = []\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            running_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            total_correct += (preds == masks).sum().item()\n",
        "            total_pixels += masks.numel()\n",
        "\n",
        "            for i in range(preds.shape[0]):\n",
        "                p_np = preds[i].cpu().numpy()\n",
        "                t_np = masks[i].cpu().numpy()\n",
        "                update_confusion_matrix(p_np, t_np, conf_mat)\n",
        "                gt_bds = extract_boundaries(t_np, num_boundaries)\n",
        "                pred_bds = extract_boundaries(p_np, num_boundaries)\n",
        "                H = t_np.shape[0]\n",
        "                bd_diff = (pred_bds - gt_bds) / float(H)\n",
        "                rmse_vals = np.sqrt(np.mean(bd_diff**2, axis=1))\n",
        "                sample_rmse_list.append(rmse_vals)\n",
        "\n",
        "    if len(sample_rmse_list) > 0:\n",
        "        boundary_rmse = np.mean(sample_rmse_list, axis=0)\n",
        "    else:\n",
        "        boundary_rmse = np.zeros(num_boundaries, dtype=np.float32)\n",
        "\n",
        "    val_loss = running_loss / len(loader)\n",
        "    pixel_acc = total_correct / total_pixels\n",
        "\n",
        "    ious, dices, mean_iou, mean_dice = compute_classwise_iou_dice(conf_mat)\n",
        "    (precs, recs, f1s,\n",
        "     macro_prec, macro_rec, macro_f1) = compute_precision_recall_f1(conf_mat)\n",
        "\n",
        "    return (val_loss, pixel_acc,\n",
        "            ious, dices, mean_iou, mean_dice,\n",
        "            precs, recs, f1s, macro_prec, macro_rec, macro_f1,\n",
        "            boundary_rmse, conf_mat)\n",
        "\n",
        "##########################################\n",
        "# 12. Create model, optimizer, and pick the loss\n",
        "##########################################\n",
        "print(f\"Selected architecture: {MODEL_ARCHITECTURE}\")\n",
        "print(f\"Selected loss function: {LOSS_TYPE}\")\n",
        "\n",
        "if MODEL_ARCHITECTURE.lower() == \"unet\":\n",
        "    model = UNet(in_channels=1, num_classes=10).to(device)\n",
        "elif MODEL_ARCHITECTURE.lower() == \"unet++\":\n",
        "    model = UNetPlusPlus(in_channels=1, num_classes=10).to(device)\n",
        "elif MODEL_ARCHITECTURE.lower() == \"unet3+\":\n",
        "    model = UNet3Plus(in_channels=1, num_classes=10).to(device)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported model architecture: {MODEL_ARCHITECTURE}\")\n",
        "\n",
        "if LOSS_TYPE == \"CE\":\n",
        "    criterion = ce_loss\n",
        "elif LOSS_TYPE == \"Dice\":\n",
        "    criterion = dice_loss\n",
        "elif LOSS_TYPE == \"Combined\":\n",
        "    criterion = lambda logits, targets: combined_loss(logits, targets, alpha=0.5)\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported LOSS_TYPE: {LOSS_TYPE}\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=45, eta_min=1e-5)\n",
        "num_epochs = 45\n",
        "best_iou = 0.0\n",
        "\n",
        "##########################################\n",
        "# 13. Decide whether to train or load\n",
        "##########################################\n",
        "if TRAIN_FROM_SCRATCH:\n",
        "    print(\"\\n=== Option set to TRAIN_FROM_SCRATCH = True. Training from scratch. ===\")\n",
        "else:\n",
        "    if (LOAD_CHECKPOINT_PATH is not None) and (os.path.isfile(LOAD_CHECKPOINT_PATH)):\n",
        "        print(f\"\\n=== Loading model from checkpoint: {LOAD_CHECKPOINT_PATH} ===\")\n",
        "        model.load_state_dict(torch.load(LOAD_CHECKPOINT_PATH, map_location=device))\n",
        "        model.eval()\n",
        "        print(\"Model loaded and set to evaluation mode. Skipping training.\")\n",
        "    else:\n",
        "        print(\"TRAIN_FROM_SCRATCH = False but checkpoint not found. Training from scratch.\")\n",
        "        TRAIN_FROM_SCRATCH = True\n",
        "\n",
        "##########################################\n",
        "# 14. Training (if requested)\n",
        "##########################################\n",
        "epoch_durations = []\n",
        "all_epoch_metrics = []\n",
        "\n",
        "if TRAIN_FROM_SCRATCH:\n",
        "    print(\"\\n=== Training Model ===\")\n",
        "    total_train_start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "        scheduler.step()\n",
        "\n",
        "        (val_loss, val_pix_acc,\n",
        "         val_ious, val_dices, val_mean_iou, val_mean_dice,\n",
        "         val_precs, val_recs, val_f1s,\n",
        "         val_macro_prec, val_macro_rec, val_macro_f1,\n",
        "         val_boundary_rmse, _) = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_durations.append(epoch_duration)\n",
        "\n",
        "        epoch_metrics = {\n",
        "            \"epoch\": epoch+1,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_pix_acc\": val_pix_acc,\n",
        "            \"val_mean_iou\": val_mean_iou,\n",
        "            \"val_mean_dice\": val_mean_dice,\n",
        "            \"val_macro_prec\": val_macro_prec,\n",
        "            \"val_macro_rec\": val_macro_rec,\n",
        "            \"val_macro_f1\": val_macro_f1\n",
        "        }\n",
        "        all_epoch_metrics.append(epoch_metrics)\n",
        "\n",
        "        print(f\"\\nEpoch [{epoch+1}/{num_epochs}]\")\n",
        "        print(f\"  Train Loss:          {train_loss:.4f}\")\n",
        "        print(f\"  Val Loss:            {val_loss:.4f}\")\n",
        "        print(f\"  Val Pixel Accuracy:  {val_pix_acc:.4f}\")\n",
        "        print(f\"  Val Mean IoU:        {val_mean_iou:.4f}\")\n",
        "        print(f\"  Val Mean Dice:       {val_mean_dice:.4f}\")\n",
        "        print(f\"  Val Macro Precision: {val_macro_prec:.4f}\")\n",
        "        print(f\"  Val Macro Recall:    {val_macro_rec:.4f}\")\n",
        "        print(f\"  Val Macro F1:        {val_macro_f1:.4f}\")\n",
        "\n",
        "        print(\"  -- Classwise Metrics --\")\n",
        "        for c_idx in range(len(val_ious)):\n",
        "            print(f\"     Class {c_idx}: IoU={val_ious[c_idx]:.4f}, \"\n",
        "                  f\"Dice={val_dices[c_idx]:.4f}, \"\n",
        "                  f\"Prec={val_precs[c_idx]:.4f}, \"\n",
        "                  f\"Rec={val_recs[c_idx]:.4f}, \"\n",
        "                  f\"F1={val_f1s[c_idx]:.4f}\")\n",
        "\n",
        "        print(\"  -- Per-Boundary RMSE (Val) --\")\n",
        "        for b_idx, rmse_val in enumerate(val_boundary_rmse):\n",
        "            print(f\"     Boundary {b_idx}: RMSE={rmse_val:.4f}\")\n",
        "\n",
        "        print(f\"  => Epoch {epoch+1} took {epoch_duration:.2f} seconds.\")\n",
        "\n",
        "        if val_mean_iou > best_iou:\n",
        "            best_iou = val_mean_iou\n",
        "            torch.save(model.state_dict(), f\"best_model_epoch_{epoch+1}.pth\")\n",
        "            print(f\"  => New best model saved with Mean IoU: {best_iou:.4f}\")\n",
        "\n",
        "    total_train_end_time = time.time()\n",
        "    total_training_time = total_train_end_time - total_train_start_time\n",
        "    print(f\"\\nTraining complete. Total training time: {total_training_time:.2f} seconds.\")\n",
        "\n",
        "    total_time_txt_path = os.path.join(TOTAL_TRAIN_TIME_OUTPUT_DIR, \"total_training_time.txt\")\n",
        "    with open(total_time_txt_path, \"w\") as f:\n",
        "        f.write(f\"Total training time (seconds): {total_training_time:.2f}\\n\")\n",
        "    print(f\"Total training time saved to: {total_time_txt_path}\")\n",
        "\n",
        "    for i, dur in enumerate(epoch_durations):\n",
        "        print(f\"Epoch {i+1} duration: {dur:.2f} seconds\")\n",
        "\n",
        "    durations_txt_path = os.path.join(DURATIONS_OUTPUT_DIR, \"epoch_durations.txt\")\n",
        "    with open(durations_txt_path, \"w\") as f:\n",
        "        f.write(\"Epoch\\tDuration_seconds\\n\")\n",
        "        for i, dur in enumerate(epoch_durations, start=1):\n",
        "            f.write(f\"{i}\\t{dur:.6f}\\n\")\n",
        "    print(f\"Epoch durations saved to: {durations_txt_path}\")\n",
        "\n",
        "    epoch_metrics_txt_path = os.path.join(EPOCH_METRICS_OUTPUT_DIR, \"epoch_metrics.txt\")\n",
        "    with open(epoch_metrics_txt_path, \"w\") as f:\n",
        "        f.write(\"Epoch\\tTrain_Loss\\tVal_Loss\\tVal_Pixel_Acc\\tVal_Mean_IoU\\tVal_Mean_Dice\\tVal_Macro_Prec\\tVal_Macro_Rec\\tVal_Macro_F1\\n\")\n",
        "        for em in all_epoch_metrics:\n",
        "            f.write(f\"{em['epoch']}\\t\"\n",
        "                    f\"{em['train_loss']:.4f}\\t\"\n",
        "                    f\"{em['val_loss']:.4f}\\t\"\n",
        "                    f\"{em['val_pix_acc']:.4f}\\t\"\n",
        "                    f\"{em['val_mean_iou']:.4f}\\t\"\n",
        "                    f\"{em['val_mean_dice']:.4f}\\t\"\n",
        "                    f\"{em['val_macro_prec']:.4f}\\t\"\n",
        "                    f\"{em['val_macro_rec']:.4f}\\t\"\n",
        "                    f\"{em['val_macro_f1']:.4f}\\n\")\n",
        "    print(f\"Epoch metrics saved to: {epoch_metrics_txt_path}\")\n",
        "\n",
        "    if len(epoch_durations) > 0:\n",
        "        plt.figure()\n",
        "        plt.plot(range(1, len(epoch_durations) + 1), epoch_durations, label='Epoch Duration (s)')\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Duration (s)\")\n",
        "        plt.title(\"Training Time per Epoch\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    torch.save(model.state_dict(), FINAL_MODEL_SAVE_PATH)\n",
        "    print(f\"Final model saved to: {FINAL_MODEL_SAVE_PATH}\")\n",
        "\n",
        "##########################################\n",
        "# 15. Validation Metrics & Standard Deviations\n",
        "##########################################\n",
        "print(\"\\n=== Validation Metrics & Standard Deviations ===\")\n",
        "(val_loss, val_pix_acc,\n",
        " val_ious, val_dices, val_mean_iou, val_mean_dice,\n",
        " val_precs, val_recs, val_f1s, val_macro_prec, val_macro_rec, val_macro_f1,\n",
        " val_boundary_rmse, _) = validate_epoch(model, val_loader, criterion)\n",
        "\n",
        "print(f\"Val Loss:            {val_loss:.4f}\")\n",
        "print(f\"Val Pixel Accuracy:  {val_pix_acc:.4f}\")\n",
        "print(f\"Val Mean IoU:        {val_mean_iou:.4f}\")\n",
        "print(f\"Val Mean Dice:       {val_mean_dice:.4f}\")\n",
        "print(f\"Val Macro Precision: {val_macro_prec:.4f}\")\n",
        "print(f\"Val Macro Recall:    {val_macro_rec:.4f}\")\n",
        "print(f\"Val Macro F1:        {val_macro_f1:.4f}\")\n",
        "for c_idx in range(len(val_ious)):\n",
        "    print(f\"  Class {c_idx}: IoU={val_ious[c_idx]:.4f}, Dice={val_dices[c_idx]:.4f}, \"\n",
        "          f\"Prec={val_precs[c_idx]:.4f}, Rec={val_recs[c_idx]:.4f}, F1={val_f1s[c_idx]:.4f}\")\n",
        "print(\"\\n-- Per-Boundary RMSE (Val) --\")\n",
        "for b_idx, rmse_val in enumerate(val_boundary_rmse):\n",
        "    print(f\"  Boundary {b_idx}: RMSE={rmse_val:.4f}\")\n",
        "\n",
        "final_val_metrics_txt = os.path.join(FINAL_METRICS_OUTPUT_DIR, \"final_validation_metrics.txt\")\n",
        "with open(final_val_metrics_txt, \"w\") as f:\n",
        "    f.write(\"Final Validation Metrics\\n\")\n",
        "    f.write(f\"Val_Loss: {val_loss:.4f}\\n\")\n",
        "    f.write(f\"Val_Pixel_Acc: {val_pix_acc:.4f}\\n\")\n",
        "    f.write(f\"Val_Mean_IoU: {val_mean_iou:.4f}\\n\")\n",
        "    f.write(f\"Val_Mean_Dice: {val_mean_dice:.4f}\\n\")\n",
        "    f.write(f\"Val_Macro_Precision: {val_macro_prec:.4f}\\n\")\n",
        "    f.write(f\"Val_Macro_Recall: {val_macro_rec:.4f}\\n\")\n",
        "    f.write(f\"Val_Macro_F1: {val_macro_f1:.4f}\\n\")\n",
        "    f.write(\"\\nClasswise IoU / Dice / Precision / Recall / F1:\\n\")\n",
        "    for c_idx in range(len(val_ious)):\n",
        "        f.write(\n",
        "            f\"Class {c_idx}\\t\"\n",
        "            f\"IoU={val_ious[c_idx]:.4f}\\t\"\n",
        "            f\"Dice={val_dices[c_idx]:.4f}\\t\"\n",
        "            f\"Prec={val_precs[c_idx]:.4f}\\t\"\n",
        "            f\"Rec={val_recs[c_idx]:.4f}\\t\"\n",
        "            f\"F1={val_f1s[c_idx]:.4f}\\n\"\n",
        "        )\n",
        "    f.write(\"\\nPer-Boundary RMSE:\\n\")\n",
        "    for b_idx, rmse_val in enumerate(val_boundary_rmse):\n",
        "        f.write(f\"Boundary {b_idx}: {rmse_val:.4f}\\n\")\n",
        "\n",
        "print(f\"Final validation metrics saved to: {final_val_metrics_txt}\")\n",
        "\n",
        "def compute_std_for_dataset(model, loader, device, num_classes=10):\n",
        "    model.eval()\n",
        "    stds_per_class = {c: [] for c in range(num_classes)}\n",
        "    overall_stds = []\n",
        "    with torch.no_grad():\n",
        "        for images, _ in loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            prob_maps = torch.softmax(outputs, dim=1)\n",
        "            prob_maps_np = prob_maps.cpu().numpy()\n",
        "            B = prob_maps_np.shape[0]\n",
        "            for i in range(B):\n",
        "                sample_prob = prob_maps_np[i]\n",
        "                overall_stds.append(np.std(sample_prob))\n",
        "                for c in range(num_classes):\n",
        "                    stds_per_class[c].append(np.std(sample_prob[c]))\n",
        "    mean_stds_per_class = {c: np.mean(stds_per_class[c]) for c in range(num_classes)}\n",
        "    mean_overall_std = np.mean(overall_stds)\n",
        "    return mean_stds_per_class, mean_overall_std\n",
        "\n",
        "val_std_per_class, val_overall_std = compute_std_for_dataset(model, val_loader, device)\n",
        "print(\"\\n=== Validation Set Standard Deviations ===\")\n",
        "for c in range(10):\n",
        "    print(f\"  Class {c}: Std = {val_std_per_class[c]:.4f}\")\n",
        "print(f\"  Overall (per sample) Std: {val_overall_std:.4f}\")\n",
        "\n",
        "#############################################\n",
        "# 16. Validation set visualization\n",
        "#############################################\n",
        "def visualize_on_batch_val(loader, model, num_boundaries=9):\n",
        "    model.eval()\n",
        "    batch_iter = iter(loader)\n",
        "    images, masks = next(batch_iter)\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        prob_maps = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "    idx = 0\n",
        "    img_np = images[idx].cpu().numpy()[0]\n",
        "    gt_np = masks[idx].cpu().numpy()\n",
        "    pred_np = preds[idx].cpu().numpy()\n",
        "    prob_np = prob_maps[idx]\n",
        "\n",
        "    H, W = img_np.shape\n",
        "    mismatch = (pred_np != gt_np)\n",
        "    prob_err_raw = np.zeros_like(gt_np, dtype=np.float32)\n",
        "    rows, cols = np.where(mismatch)\n",
        "    if rows.size > 0:\n",
        "        pred_classes = pred_np[mismatch]\n",
        "        prob_err_raw[mismatch] = prob_np[pred_classes, rows, cols]\n",
        "\n",
        "    gt_bds = extract_boundaries(gt_np, num_boundaries)\n",
        "    raw_bds = extract_boundaries(pred_np, num_boundaries)\n",
        "\n",
        "    def show_image(ax, data_2d, title, cmap=None, vmin=None, vmax=None):\n",
        "        flipped = np.flipud(data_2d)\n",
        "        ax.set_title(title)\n",
        "        ax.imshow(flipped, extent=[0, W, 0, H], origin='lower',\n",
        "                  cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "        ax.set_xlim(0, W)\n",
        "        ax.set_ylim(0, H)\n",
        "        ax.set_xlabel(\"X (px)\")\n",
        "        ax.set_ylabel(\"Y (px)\")\n",
        "\n",
        "    fig = plt.figure(figsize=(12,32))\n",
        "\n",
        "    ax1 = fig.add_subplot(6,1,1)\n",
        "    show_image(ax1, img_np, \"1) Validation Input (OCT)\", cmap=\"gray\")\n",
        "\n",
        "    ax2 = fig.add_subplot(6,1,2)\n",
        "    show_image(ax2, gt_np, \"2) GT Mask\", cmap=custom_cmap)\n",
        "    gt_patches = [\n",
        "        Patch(facecolor=custom_colors[i], edgecolor='black', label=custom_class_names[i])\n",
        "        for i in range(len(custom_colors))\n",
        "    ]\n",
        "    ax2.legend(handles=gt_patches, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    ax3 = fig.add_subplot(6,1,3)\n",
        "    show_image(ax3, img_np, \"3) GT Boundaries\", cmap=\"gray\")\n",
        "    for b_idx in range(gt_bds.shape[0]):\n",
        "        boundary_ys = H - gt_bds[b_idx]\n",
        "        ax3.plot(np.arange(W), boundary_ys,\n",
        "                 color=custom_boundary_colors[b_idx % len(custom_boundary_colors)],\n",
        "                 linestyle='-', linewidth=1.25, alpha=0.7)\n",
        "\n",
        "    ax4 = fig.add_subplot(6,1,4)\n",
        "    show_image(ax4, pred_np, \"4) Raw Prediction\", cmap=custom_cmap)\n",
        "\n",
        "    ax5 = fig.add_subplot(6,1,5)\n",
        "    show_image(ax5, img_np, \"5) Raw Predicted Boundaries\", cmap=\"gray\")\n",
        "    for b_idx in range(raw_bds.shape[0]):\n",
        "        boundary_ys = H - raw_bds[b_idx]\n",
        "        ax5.plot(np.arange(W), boundary_ys,\n",
        "                 color=custom_boundary_colors[b_idx % len(custom_boundary_colors)],\n",
        "                 linestyle='--', linewidth=1.875, alpha=0.8)\n",
        "\n",
        "    ax6 = fig.add_subplot(6,1,6)\n",
        "    show_image(ax6, prob_err_raw, \"6) Raw Prediction Probability Heatmap\",\n",
        "               cmap='jet', vmin=0, vmax=1)\n",
        "    plt.colorbar(ax6.images[0], ax=ax6, label=\"Probability Error\",\n",
        "                 fraction=0.046, pad=0.02, shrink=0.375)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Validation Set Visualization ---\")\n",
        "if len(val_loader) > 0:\n",
        "    visualize_on_batch_val(val_loader, model, num_boundaries=9)\n",
        "\n",
        "##########################################\n",
        "# 17. Compute Boundary Difference for Validation\n",
        "##########################################\n",
        "def compute_boundary_diff_across_dataset(model, loader, device, num_boundaries=9):\n",
        "    boundary_diffs_raw = [[] for _ in range(num_boundaries)]\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, masks in loader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            for i in range(preds.shape[0]):\n",
        "                pred_np = preds[i].cpu().numpy()\n",
        "                gt_np = masks[i].cpu().numpy()\n",
        "                gt_bds = extract_boundaries(gt_np, num_boundaries)\n",
        "                pred_bds = extract_boundaries(pred_np, num_boundaries)\n",
        "                H = gt_np.shape[0]\n",
        "                raw_diff = (pred_bds - gt_bds) / float(H) * 100.0\n",
        "                for b_idx in range(num_boundaries):\n",
        "                    boundary_diffs_raw[b_idx].append(raw_diff[b_idx, :])\n",
        "    return boundary_diffs_raw\n",
        "\n",
        "def compute_overall_mean_signal(boundary_diffs):\n",
        "    if not boundary_diffs or not boundary_diffs[0]:\n",
        "        return None\n",
        "    import numpy as np\n",
        "    num_boundaries = len(boundary_diffs)\n",
        "    num_samples = len(boundary_diffs[0])\n",
        "    sample_mean_signals = []\n",
        "    for i in range(num_samples):\n",
        "        stacked = np.stack([boundary_diffs[b][i] for b in range(num_boundaries)], axis=0)\n",
        "        mean_across_boundaries = np.mean(stacked, axis=0)\n",
        "        sample_mean_signals.append(mean_across_boundaries)\n",
        "    sample_mean_signals = np.stack(sample_mean_signals, axis=0)\n",
        "    overall_mean = np.mean(sample_mean_signals, axis=0)\n",
        "    return overall_mean\n",
        "\n",
        "def plot_overall_boundary_diff_signal(overall_mean, title, custom_x_limits=None, custom_y_limits=None,\n",
        "                                      output_dir=SIGNAL_ERROR_OUTPUT_DIR):\n",
        "    import numpy as np\n",
        "    if overall_mean is None:\n",
        "        print(\"No data to plot.\")\n",
        "        return\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    x = np.arange(overall_mean.shape[0])\n",
        "    fig, ax = plt.subplots(figsize=(9.6, 4.8))\n",
        "    ax.plot(x, overall_mean, label=\"Overall Mean Boundary Error\", alpha=0.9)\n",
        "    ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.8)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Column Index (px)\")\n",
        "    ax.set_ylabel(\"Error (%)\")\n",
        "    if custom_x_limits is not None:\n",
        "        ax.set_xlim(*custom_x_limits)\n",
        "    else:\n",
        "        ax.set_xlim(0, overall_mean.shape[0])\n",
        "    if custom_y_limits is not None:\n",
        "        ax.set_ylim(*custom_y_limits)\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    data = np.vstack((x, overall_mean)).T\n",
        "    filename = os.path.join(output_dir, title.replace(\" \", \"_\") + \".txt\")\n",
        "    np.savetxt(filename, data, fmt=\"%.6f\", header=\"Column_Index Overall_Mean_Error (%)\")\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def plot_boundary_diff_signals(boundary_diffs, title_prefix=\"Boundary Error Signals\",\n",
        "                               custom_x_limits=None, custom_y_limits=None,\n",
        "                               output_dir=SIGNAL_ERROR_OUTPUT_DIR):\n",
        "    import numpy as np\n",
        "    num_boundaries = len(boundary_diffs)\n",
        "    if not boundary_diffs[0]:\n",
        "        print(\"No data to plot.\")\n",
        "        return\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    sample_width = boundary_diffs[0][0].shape[0]\n",
        "    x = np.arange(sample_width)\n",
        "    fig, ax = plt.subplots(figsize=(9.6, 4.8))\n",
        "    for b in range(num_boundaries):\n",
        "        error_signals = np.stack(boundary_diffs[b], axis=0)\n",
        "        mean_error = np.mean(error_signals, axis=0)\n",
        "        ax.plot(x, mean_error,\n",
        "                label=f\"Boundary {b}\",\n",
        "                color=custom_boundary_colors[b % len(custom_boundary_colors)],\n",
        "                alpha=0.9)\n",
        "        data = np.vstack((x, mean_error)).T\n",
        "        filename = os.path.join(output_dir,\n",
        "                                f\"{title_prefix.replace(' ', '_')}_Boundary_{b}.txt\")\n",
        "        np.savetxt(filename, data, fmt=\"%.6f\", header=\"Column_Index Error (%)\")\n",
        "        print(f\"Data for Boundary {b} saved to {filename}\")\n",
        "    ax.axhline(0, color='black', linestyle='--', linewidth=1, alpha=0.8)\n",
        "    ax.set_title(title_prefix)\n",
        "    ax.set_xlabel(\"Column Index (px)\")\n",
        "    ax.set_ylabel(\"Error (%)\")\n",
        "    if custom_x_limits is not None:\n",
        "        ax.set_xlim(*custom_x_limits)\n",
        "    else:\n",
        "        ax.set_xlim(0, sample_width)\n",
        "    if custom_y_limits is not None:\n",
        "        ax.set_ylim(*custom_y_limits)\n",
        "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n--- Validation Set Boundary Error Signal ---\")\n",
        "if len(val_loader) > 0:\n",
        "    val_boundary_diffs_raw = compute_boundary_diff_across_dataset(model, val_loader, device, num_boundaries=9)\n",
        "    plot_boundary_diff_signals(val_boundary_diffs_raw,\n",
        "                               title_prefix=\"Validation Set Boundary Error Signal (Raw)\",\n",
        "                               custom_x_limits=VALIDATION_CUSTOM_X_LIMITS,\n",
        "                               custom_y_limits=VALIDATION_CUSTOM_Y_LIMITS,\n",
        "                               output_dir=SIGNAL_ERROR_OUTPUT_DIR)\n",
        "\n",
        "    val_overall_raw = compute_overall_mean_signal(val_boundary_diffs_raw)\n",
        "    plot_overall_boundary_diff_signal(\n",
        "        val_overall_raw,\n",
        "        \"Validation Set Overall Boundary Error Signal (Raw)\",\n",
        "        custom_x_limits=VALIDATION_OVERALL_CUSTOM_X_LIMITS,\n",
        "        custom_y_limits=VALIDATION_OVERALL_CUSTOM_Y_LIMITS,\n",
        "        output_dir=SIGNAL_ERROR_OUTPUT_DIR\n",
        "    )\n",
        "\n",
        "##########################################\n",
        "# 18. Test Set Evaluation & Visualization\n",
        "##########################################\n",
        "test_results = validate_epoch(model, test_loader, criterion, num_classes=10, num_boundaries=9)\n",
        "(test_loss, test_pix_acc,\n",
        " test_ious, test_dices, test_mean_iou, test_mean_dice,\n",
        " test_precs, test_recs, test_f1s,\n",
        " test_mprec, test_mrec, test_mf1,\n",
        " test_bd_rmse, test_conf_mat) = test_results\n",
        "\n",
        "print(\"\\n=== Test Set Evaluation ===\")\n",
        "print(f\"Test Loss:           {test_loss:.4f}\")\n",
        "print(f\"Pixel Accuracy:      {test_pix_acc:.4f}\")\n",
        "print(f\"Mean IoU:            {test_mean_iou:.4f}\")\n",
        "print(f\"Mean Dice:           {test_mean_dice:.4f}\")\n",
        "print(f\"Macro Precision:     {test_mprec:.4f}\")\n",
        "print(f\"Macro Recall:        {test_mrec:.4f}\")\n",
        "print(f\"Macro F1:            {test_mf1:.4f}\")\n",
        "\n",
        "for c_idx in range(len(test_ious)):\n",
        "    print(f\"  Class {c_idx}: IoU={test_ious[c_idx]:.4f}, Dice={test_dices[c_idx]:.4f}, \"\n",
        "          f\"Prec={test_precs[c_idx]:.4f}, Rec={test_recs[c_idx]:.4f}, F1={test_f1s[c_idx]:.4f}\")\n",
        "\n",
        "print(\"\\n-- Per-Boundary RMSE (Test Set) --\")\n",
        "for b_idx, rmse_val in enumerate(test_bd_rmse):\n",
        "    print(f\"  Boundary {b_idx}: RMSE={rmse_val:.4f}\")\n",
        "\n",
        "test_std_per_class, test_overall_std = compute_std_for_dataset(model, test_loader, device)\n",
        "print(\"\\n=== Test Set Standard Deviations ===\")\n",
        "for c in range(10):\n",
        "    print(f\"  Class {c}: Std = {test_std_per_class[c]:.4f}\")\n",
        "print(f\"  Overall (per sample) Std: {test_overall_std:.4f}\")\n",
        "\n",
        "test_metrics_txt = os.path.join(FINAL_METRICS_OUTPUT_DIR, \"final_test_metrics.txt\")\n",
        "with open(test_metrics_txt, \"w\") as f:\n",
        "    f.write(\"Final Test Metrics\\n\")\n",
        "    f.write(f\"Test_Loss: {test_loss:.4f}\\n\")\n",
        "    f.write(f\"Test_Pixel_Acc: {test_pix_acc:.4f}\\n\")\n",
        "    f.write(f\"Test_Mean_IoU: {test_mean_iou:.4f}\\n\")\n",
        "    f.write(f\"Test_Mean_Dice: {test_mean_dice:.4f}\\n\")\n",
        "    f.write(f\"Test_Macro_Precision: {test_mprec:.4f}\\n\")\n",
        "    f.write(f\"Test_Macro_Recall: {test_mrec:.4f}\\n\")\n",
        "    f.write(f\"Test_Macro_F1: {test_mf1:.4f}\\n\")\n",
        "    f.write(\"\\nClasswise IoU / Dice / Precision / Recall / F1:\\n\")\n",
        "    for c_idx in range(len(test_ious)):\n",
        "        f.write(\n",
        "            f\"Class {c_idx}\\t\"\n",
        "            f\"IoU={test_ious[c_idx]:.4f}\\t\"\n",
        "            f\"Dice={test_dices[c_idx]:.4f}\\t\"\n",
        "            f\"Prec={test_precs[c_idx]:.4f}\\t\"\n",
        "            f\"Rec={test_recs[c_idx]:.4f}\\t\"\n",
        "            f\"F1={test_f1s[c_idx]:.4f}\\n\"\n",
        "        )\n",
        "    f.write(\"\\nPer-Boundary RMSE:\\n\")\n",
        "    for b_idx, rmse_val in enumerate(test_bd_rmse):\n",
        "        f.write(f\"Boundary {b_idx}: {rmse_val:.4f}\\n\")\n",
        "\n",
        "print(f\"Final test metrics saved to: {test_metrics_txt}\")\n",
        "\n",
        "##########################################\n",
        "# 18.1 Single-Batch Visualization for the Test Set\n",
        "##########################################\n",
        "def visualize_on_batch_test(loader, model, num_boundaries=9):\n",
        "    model.eval()\n",
        "    batch_iter = iter(loader)\n",
        "    images, masks = next(batch_iter)\n",
        "    images, masks = images.to(device), masks.to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(images)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        prob_maps = torch.softmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "    idx = 0\n",
        "    img_np = images[idx].cpu().numpy()[0]\n",
        "    gt_np = masks[idx].cpu().numpy()\n",
        "    pred_np = preds[idx].cpu().numpy()\n",
        "    prob_np = prob_maps[idx]\n",
        "\n",
        "    H, W = img_np.shape\n",
        "    mismatch = (pred_np != gt_np)\n",
        "    prob_err_raw = np.zeros_like(gt_np, dtype=np.float32)\n",
        "    rows, cols = np.where(mismatch)\n",
        "    if rows.size > 0:\n",
        "        pred_classes = pred_np[mismatch]\n",
        "        prob_err_raw[mismatch] = prob_np[pred_classes, rows, cols]\n",
        "\n",
        "    gt_bds = extract_boundaries(gt_np, num_boundaries)\n",
        "    raw_bds = extract_boundaries(pred_np, num_boundaries)\n",
        "\n",
        "    def show_image(ax, data_2d, title, cmap=None, vmin=None, vmax=None):\n",
        "        flipped = np.flipud(data_2d)\n",
        "        ax.set_title(title)\n",
        "        ax.imshow(flipped, extent=[0, W, 0, H], origin='lower',\n",
        "                  cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "        ax.set_xlim(0, W)\n",
        "        ax.set_ylim(0, H)\n",
        "        ax.set_xlabel(\"X (px)\")\n",
        "        ax.set_ylabel(\"Y (px)\")\n",
        "\n",
        "    fig = plt.figure(figsize=(12,32))\n",
        "\n",
        "    ax1 = fig.add_subplot(6,1,1)\n",
        "    show_image(ax1, img_np, \"1) Test Input (OCT)\", cmap=\"gray\")\n",
        "\n",
        "    ax2 = fig.add_subplot(6,1,2)\n",
        "    show_image(ax2, gt_np, \"2) GT Mask\", cmap=custom_cmap)\n",
        "    gt_patches = [\n",
        "        Patch(facecolor=custom_colors[i], edgecolor='black', label=custom_class_names[i])\n",
        "        for i in range(len(custom_colors))\n",
        "    ]\n",
        "    ax2.legend(handles=gt_patches, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    ax3 = fig.add_subplot(6,1,3)\n",
        "    show_image(ax3, img_np, \"3) GT Boundaries\", cmap=\"gray\")\n",
        "    for b_idx in range(gt_bds.shape[0]):\n",
        "        boundary_ys = H - gt_bds[b_idx]\n",
        "        ax3.plot(np.arange(W), boundary_ys,\n",
        "                 color=custom_boundary_colors[b_idx % len(custom_boundary_colors)],\n",
        "                 linestyle='-', linewidth=1.25, alpha=0.7)\n",
        "\n",
        "    ax4 = fig.add_subplot(6,1,4)\n",
        "    show_image(ax4, pred_np, \"4) Raw Prediction\", cmap=custom_cmap)\n",
        "\n",
        "    ax5 = fig.add_subplot(6,1,5)\n",
        "    show_image(ax5, img_np, \"5) Raw Predicted Boundaries\", cmap=\"gray\")\n",
        "    for b_idx in range(raw_bds.shape[0]):\n",
        "        boundary_ys = H - raw_bds[b_idx]\n",
        "        ax5.plot(np.arange(W), boundary_ys,\n",
        "                 color=custom_boundary_colors[b_idx % len(custom_boundary_colors)],\n",
        "                 linestyle='--', linewidth=1.875, alpha=0.8)\n",
        "\n",
        "    ax6 = fig.add_subplot(6,1,6)\n",
        "    show_image(ax6, prob_err_raw, \"6) Raw Prediction Probability Heatmap\",\n",
        "               cmap='jet', vmin=0, vmax=1)\n",
        "    plt.colorbar(ax6.images[0], ax=ax6, label=\"Probability Error\",\n",
        "                 fraction=0.046, pad=0.02, shrink=0.375)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n=== Single-Batch Visualization on the Test Set ===\")\n",
        "if len(test_loader) > 0:\n",
        "    visualize_on_batch_test(test_loader, model, num_boundaries=9)\n",
        "\n",
        "##########################################\n",
        "# 18.2 Plotting Test Set Boundary Errors\n",
        "##########################################\n",
        "test_boundary_diffs_raw = compute_boundary_diff_across_dataset(model, test_loader, device, num_boundaries=9)\n",
        "\n",
        "plot_boundary_diff_signals(test_boundary_diffs_raw,\n",
        "                           title_prefix=\"Test Set Boundary Error Signal (Raw)\",\n",
        "                           custom_x_limits=TEST_CUSTOM_X_LIMITS,\n",
        "                           custom_y_limits=TEST_CUSTOM_Y_LIMITS,\n",
        "                           output_dir=SIGNAL_ERROR_OUTPUT_DIR)\n",
        "\n",
        "test_overall_raw = compute_overall_mean_signal(test_boundary_diffs_raw)\n",
        "plot_overall_boundary_diff_signal(\n",
        "    test_overall_raw,\n",
        "    \"Test Set Overall Boundary Error Signal (Raw)\",\n",
        "    custom_x_limits=TEST_OVERALL_CUSTOM_X_LIMITS,\n",
        "    custom_y_limits=TEST_OVERALL_CUSTOM_Y_LIMITS,\n",
        "    output_dir=SIGNAL_ERROR_OUTPUT_DIR\n",
        ")\n",
        "\n",
        "print(\"\\nDone.\")"
      ]
    }
  ]
}